i want to create a credit risk model but using the transformer architecture to learn from the transactions of each client.
the first part is to create a stringfied representation of the transaction and then tokenize it. an example of a stringfied transaction would be "<out><amount:0-100><hour:10><day:5><month:7><day_week:sunday>netflix.com". but then i need to concatenate a series of transactions and probably should use a separator like "<EOT>" that would mean "end of transaction" and also for the full sequence i believe would be necessary to add a "<SOS>" start of sequence token and a "<EOS>" end of sequence and cannot forget the "<PAD>" to make the sequence from each client have the same length. you can correct me if this is not the way to do it...
i also need to tokenize these sequence of transactions and i was thinking about using the BPE method, probably using the hugging face tokenizers with a custom vocabulary.
the next step would be the transformer, that from all my studies would be a decoder-only for the pre-training optimized for NTP(next token prediction) and then i could add a MLP or other architecture to be able to fine-tune it to a classification problem(credit risk modeling).